
# ONNX Runtime Web Example on CPU and GPU

This project demonstrates the performance differences between running ONNX Runtime inference sessions on CPU and GPU in a web environment. The example uses ONNX Runtime Web and OpenCV.js to highlight the efficiency and speed benefits of GPU acceleration.

## Features

- **ONNX Runtime Web Integration**: Utilizes ONNX Runtime Web to run machine learning models directly in the browser.
- **CPU and GPU Comparison**: Provides a comparison of inference speeds between CPU and GPU.
- **OpenCV.js**: Uses OpenCV.js for image processing tasks.
- **Performance Metrics**: Displays elapsed time for inference to illustrate performance differences.

## Getting Started

### Prerequisites

- A modern web browser (e.g., Chrome, Firefox)
- Basic knowledge of JavaScript and HTML

### Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/shimaamorsy/ONNX_Runtime_Web_Example_on_GPU_and_GPU.git
   cd ONNX_Runtime_Web_Example_on_GPU_and_GPU
2-Open the project directory in your preferred code editor.
3-Start a local server. You can use Python's built-in HTTP server for this:
     ```bash
     python3 -m http.server
This will start a server on port 8000 by default.

4.Open your web browser and navigate to http://localhost:8000.

5.Explore the project and compare the performance of ONNX Runtime inference on CPU and GPU.

     
