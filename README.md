
# ONNX Runtime Web Example on CPU and GPU

This project demonstrates the performance differences between running ONNX Runtime inference sessions on CPU and GPU in a web environment. The example uses ONNX Runtime Web and OpenCV.js to highlight the efficiency and speed benefits of GPU acceleration.

## Features

- **ONNX Runtime Web Integration**: Utilizes ONNX Runtime Web to run machine learning models directly in the browser.
- **CPU and GPU Comparison**: Provides a comparison of inference speeds between CPU and GPU.
- **OpenCV.js**: Uses OpenCV.js for image processing tasks.
- **Performance Metrics**: Displays elapsed time for inference to illustrate performance differences.

## Getting Started

### Prerequisites

- A modern web browser (e.g., Chrome, Firefox)
- Basic knowledge of JavaScript and HTML

### Setup

1. **Open the project directory in your preferred code editor.**

2. **Start a local server:**
   - You can use Python's built-in HTTP server for this:
     ```bash
     python3 -m http.server
     ```
     This will start a server on port 8000 by default.

3. **Open your web browser and navigate to [http://localhost:8000](http://localhost:8000).**

4. **Explore the project and compare the performance of ONNX Runtime inference on CPU and GPU.**

## Contributing

Contributions are welcome! Please fork the repository, make your changes, and submit a pull request.

## License
